{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ScaryStories\", 'rb') as picklefile: \n",
    "    stories = pkl.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA/NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays topics nicely\n",
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to add stop words to \"english\" \n",
    "def addStopWords(words):\n",
    "    return text.ENGLISH_STOP_WORDS.union(words)\n",
    "\n",
    "stop_words = addStopWords(['like', 'know', 'don', 'just', 'got', 'day', 'said', 'room', 'went',\n",
    "                           'amp', 'time', 'just', 'like', 'com', 'https', 'said', 'thought', 'catalog',\n",
    "                           'home', 'car', 'didn', 'house', 'door', 'weekly', 'food', 'best', 'friday', 'couldn',\n",
    "                           'inbox', 'stories', 'week', 'connection', 'series', 'privacy', 'terms', 'weren', 'japan',\n",
    "                           'aren', 'wasn'\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulls out nouns and adjectives from documents in order to filter important words\n",
    "def nouns_adj(text):\n",
    "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
    "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
    "    tokenized = word_tokenize(text)\n",
    "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
    "    return ' '.join(nouns_adj)\n",
    "\n",
    "filteredStories = []\n",
    "for story in stories:\n",
    "    new = nouns_adj(story)\n",
    "    filteredStories.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays topics and returns array of documents by topic percentages with easy to edit parameters\n",
    "# parameters to edit are range of topic words, number of topics, lsa or nmf, count vectorizer or tfidf,\n",
    "# documents with all words or just nouns and adjectives, and min and max times a topic word is allowed to appear\n",
    "def makeVecShowTopics(ngram_range, numTopics, model='lsa', vecType='count_vectorizer', stop_words=stop_words, min_df=1, max_df=1.0, stories=stories):\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words=stop_words,\n",
    "                                       token_pattern=\"\\\\b[a-z][a-z]+\\\\b\", min_df=min_df, max_df=max_df)\n",
    "    tfidf = TfidfVectorizer(ngram_range=ngram_range, stop_words=stop_words, token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "                            min_df=min_df, max_df=max_df)\n",
    "    \n",
    "    if model != 'nmf':\n",
    "        if vecType == 'tfidf':\n",
    "            dtm = tfidf.fit_transform(stories)\n",
    "            lsa = TruncatedSVD(n_components=numTopics)\n",
    "            lsaVec = lsa.fit_transform(dtm)\n",
    "            display_topics(lsa, tfidf.get_feature_names(), 8)\n",
    "            return lsaVec\n",
    "        else:\n",
    "            dtm = count_vectorizer.fit_transform(stories)\n",
    "            lsa = TruncatedSVD(numTopics)\n",
    "            lsaVec = lsa.fit_transform(dtm)\n",
    "            display_topics(lsa, count_vectorizer.get_feature_names(), 8)\n",
    "            return lsaVec\n",
    "    else:\n",
    "        if vecType == 'tfidf':\n",
    "            dtm = tfidf.fit_transform(stories)\n",
    "            nmf = NMF(n_components=numTopics)\n",
    "            nmfVec = nmf.fit_transform(dtm)\n",
    "            display_topics(nmf, tfidf.get_feature_names(), 8)\n",
    "            return nmfVec\n",
    "        else:\n",
    "            dtm = count_vectorizer.fit_transform(stories)\n",
    "            nmf = NMF(numTopics)\n",
    "            nmfVec = nmf.fit_transform(dtm)\n",
    "            display_topics(nmf, count_vectorizer.get_feature_names(), 8)\n",
    "            return nmfVec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "driver, cars, shirt, bus, ear, neck, seat, super\n",
      "\n",
      "Topic  1\n",
      "table, water, strange, regular, sun, yard, moment, bedroom\n",
      "\n",
      "Topic  2\n",
      "apartment, ghost, girlfriend, word, air, father, chair, conversation\n",
      "\n",
      "Topic  3\n",
      "grandma, picture, downstairs, alive, father, closet, shut, able\n",
      "\n",
      "Topic  4\n",
      "office, mommy, plane, heart, class, fine, dogs, hallway\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.25303857e-02, 1.23495477e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       [3.43243266e-02, 0.00000000e+00, 1.54401895e-02, 0.00000000e+00,\n",
       "        1.01653514e-02],\n",
       "       [0.00000000e+00, 1.10115006e-02, 5.07609679e-01, 0.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       ...,\n",
       "       [5.75508714e-02, 1.01113496e-02, 8.06638083e-02, 6.57740415e-02,\n",
       "        0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 1.52689116e-02, 8.77937905e-03,\n",
       "        2.15195221e-01],\n",
       "       [9.73002230e-03, 0.00000000e+00, 5.20710090e-05, 0.00000000e+00,\n",
       "        2.80069065e-01]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# edit parameters accordingly and find best topic models\n",
    "makeVecShowTopics((1,1), 5, model='nmf', vecType='tfidf', min_df=6, max_df=10, stories=filteredStories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "mommy, daddy, men, cousin, coworker, boy, park, little girl\n",
      "\n",
      "Topic  1\n",
      "plane, radio, wreck, roommate, today, north, market, state\n",
      "\n",
      "Topic  2\n",
      "serial, killer, serial killer, murders, payphone, scariest, calls, hospital\n",
      "\n",
      "Topic  0\n",
      "guys, construction, park, trip, fish, distance, grass, city\n",
      "\n",
      "Topic  1\n",
      "student, students, staff, afternoon, safety, deserted, answer, rooms\n",
      "\n",
      "Topic  2\n",
      "letters, sign, gun, sheriff, dumb, writer, department, box\n",
      "\n",
      "Topic  3\n",
      "arm, live, ambulance, porch, neighborhood, fence, scream, injury\n",
      "\n",
      "Topic  0\n",
      "guy, way, road, man, truck, night, window, work\n",
      "\n",
      "Topic  1\n",
      "board, ouija, friend, spirit, questions, friends, plane, experience\n",
      "\n",
      "Topic  2\n",
      "dream, light, friend, wall, sleep, white, thing, previous\n",
      "\n",
      "Topic  3\n",
      "old, brother, year, daughter, kitchen, daddy, open, friend\n",
      "\n",
      "Topic  4\n",
      "mom, grandma, picture, phone, sister, bathroom, dog, grandparents\n",
      "\n",
      "Topic  5\n",
      "girl, dad, little, school, bed, thing, son, people\n",
      "\n",
      "Topic  0\n",
      "driver, building, apartment, cars, super, glass, entire, store\n",
      "\n",
      "Topic  1\n",
      "kitchen, single, bedroom, knife, table, aunt, roommate, places\n",
      "\n",
      "Topic  2\n",
      "daughter, sleep, mommy, heart, coworker, baby, kid, imagination\n",
      "\n",
      "Topic  3\n",
      "ouija, spirit, questions, experience, cousin, strange, details, word\n",
      "\n",
      "Topic  4\n",
      "cat, dog, dogs, teeth, huge, book, explanation, movie\n"
     ]
    }
   ],
   "source": [
    "# best topic models\n",
    "threeTopics = makeVecShowTopics((1,2), 3, model='nmf', vecType='tfidf', min_df=2, max_df=6, stories=filteredStories)\n",
    "fourTopics = makeVecShowTopics((1,2), 4, model='nmf', min_df=2, max_df=6, stories=filteredStories)\n",
    "sixTopics = makeVecShowTopics((1,1), 6, model='nmf', vecType='tfidf', min_df=3, stories=filteredStories)\n",
    "fiveTopics = makeVecShowTopics((1,1), 5, model='nmf', vecType='tfidf', min_df=5, max_df=12, stories=filteredStories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle arrays of topic probabilities (could have done this as list of lists)\n",
    "with open('sixTopics', 'wb') as picklefile:\n",
    "    pkl.dump(sixTopics, picklefile)\n",
    "\n",
    "with open('threeTopics', 'wb') as picklefile:\n",
    "    pkl.dump(threeTopics, picklefile)\n",
    "    \n",
    "with open('fourTopics', 'wb') as picklefile:\n",
    "    pkl.dump(fourTopics, picklefile)\n",
    "    \n",
    "with open('fiveTopics', 'wb') as picklefile:\n",
    "    pkl.dump(fiveTopics, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer for parsing/counting words\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1),  \n",
    "                                   stop_words='english', token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "\n",
    "tfidf.fit(filteredStories)\n",
    "\n",
    "counts = tfidf.transform(filteredStories).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map number ID to word\n",
    "id2word = dict((v,k) for k, v in tfidf.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.003*\"house\" + 0.003*\"room\" + 0.002*\"time\" + 0.002*\"night\" + 0.002*\"dream\" + 0.002*\"light\" + 0.002*\"years\" + 0.002*\"door\" + 0.002*\"tv\" + 0.002*\"cat\"'),\n",
       " (1,\n",
       "  '0.002*\"board\" + 0.001*\"spirit\" + 0.001*\"car\" + 0.001*\"ground\" + 0.001*\"thought\" + 0.001*\"wife\" + 0.001*\"catalog\" + 0.001*\"closet\" + 0.001*\"chris\" + 0.001*\"cream\"'),\n",
       " (2,\n",
       "  '0.003*\"car\" + 0.003*\"old\" + 0.003*\"door\" + 0.003*\"guy\" + 0.003*\"friend\" + 0.003*\"room\" + 0.002*\"day\" + 0.002*\"home\" + 0.002*\"bed\" + 0.002*\"year\"')]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lda experimenting - not going to work\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=3, id2word=id2word, passes=10)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
